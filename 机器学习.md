 # 有监督学习
 ## 线性回归
 ## 正则化

 ## 逻辑回归
 有监督学习的分类任务
 ## 支持向量机（support vector machine,SVM）
 可用于分类、回归
 ## 支持向量机核方法
 ## 朴素贝叶斯
 常用于自然语言分类如垃圾邮件过滤
 ## 决策树
 - 决策树模型核心是下面几部分：
    + 结点和有向边组成。
    + 结点有内部结点和叶结点2种类型。
    + 内部结点表示一个特征，叶结点表示一个类。
 - 主流的决策树算法有：
    + ID3：基于信息增益来选择分裂属性（每步选择信息增益最大的属性作为分裂节点，树可能是多叉的）。
    + C4.5：基于信息增益率来选择分裂属性（每步选择信息增益率最大的属性作为分裂节点，树可能是多叉的）。
    + CART：基于基尼系数来构建决策树（每步要求基尼系数最小，树是二叉的）。
    >其中：CART树全称Classification And Regression Tree，即可以用于分类，也可以用于回归，这里指的回归树就是 CART 树，ID3和C4.5不能用于回归问题。
 ## 随机森林
 >是将多个模型综合起来创建更高性能模型的方法，可用于回归、分类.随机森林的目标是利用多个决策树模型，获得比单个决策树更高的预测精度
 >
- （1）随机森林优点
1. 对于高维（特征很多）稠密型的数据适用，不用降维，无需做特征选择。
2. 构建随机森林模型的过程，亦可帮助判断特征的重要程度。
3. 可以借助模型构建组合特征。
4. 并行集成，有效控制过拟合。
5. 工程实现并行简单，训练速度快。
6. 对于不平衡的数据集友好，可以平衡误差。
7. 对于特征确实鲁棒性强，可以维持不错的准确度。
- （2）随机森林缺点
1. 在噪声过大的分类和回归数据集上还是可能会过拟合。
2. 相比单一决策树，因其随机性，模型解释会更复杂一些。
## GBDT
>GBDT（Gradient Boosting Decision Tree），全名叫梯度提升决策树，是一种迭代的决策树算法，又叫 MART（Multiple Additive Regression Tree），它通过构造一组弱的学习器（树），并把多颗决策树的结果累加起来作为最终的预测输出。该算法将决策树与集成思想进行了有效的结合。
- 1）优点
1. 预测阶段，因为每棵树的结构都已确定，可并行化计算，计算速度快。
2. 适用稠密数据，泛化能力和表达能力都不错，数据科学竞赛榜首常见模型。
3. 可解释性不错，鲁棒性亦可，能够自动发现特征间的高阶关系。
- 2）缺点
1. GBDT 在高维稀疏的数据集上，效率较差，且效果表现不如 SVM 或神经网络。
2. 适合数值型特征，在 NLP 或文本特征上表现弱。
3.训练过程无法并行，工程加速只能体现在单颗树构建过程中
- 随机森林 VS GDBT
    + 1）相同点
        1.都是集成模型，由多棵树组构成，最终的结果都是由多棵树一起决定。
        2.RF 和 GBDT 在使用 CART 树时，可以是分类树或者回归树。
    + 2）不同点
        1. 训练过程中，随机森林的树可以并行生成，而 GBDT 只能串行生成。
        2. 随机森林的结果是多数表决表决的，而 GBDT 则是多棵树累加之。
        3. 随机森林对异常值不敏感，而 GBDT 对异常值比较敏感。
        4. 随机森林降低模型的方差，而 GBDT 是降低模型的偏差。
 ## 神经网络
 ## KNN(K近邻)
 >可用于分类、回归。KNN在训练时机械的记住所有数据，在对未知数据进行分类时，KNN计算未知数据与训练数据的距离通过多数表决找到最近邻的k个点然后分类。
 >适用于具有复杂边界的数据.
 >当数据量较小或维度较小时，KNN效果很好，但是数据量或维度较大时，就需要考虑其他方法
 # 无监督学习
 ## PCA主成分分析
 降维算法
 ## 
 ## 
 ## 
 ## 